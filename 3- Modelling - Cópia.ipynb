{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37071862-8fac-4166-892a-88fc0ed59368",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5ec415cb6a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{dependency}: {e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m     \u001b[1;31m# NOTE: to be revisited following future namespace cleanup.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;31m# See gh-14454 and gh-15672 for discussion.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\numpy\\lib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnpyio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0marrayterator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArrayterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0marraypad\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multiarray_umath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtracemalloc_domain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['axes.labelsize'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047dcd1-4066-4666-9caf-0e866b4c6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb =pd.read_csv(\"C:/Users/Madalena Nunes/OneDrive/Ambiente de Trabalho/Business Analytics/Tese/data/cleaned_dataEDA.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3d3a9-148b-493e-8ddb-0a5a6b22300f",
   "metadata": {},
   "source": [
    "# 1.Preparing data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b3955-5528-43b0-b588-6f30f5f19020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f3381b0-a92d-40dc-9cf7-d6286fb7f968",
   "metadata": {},
   "source": [
    "### Dropping columns and assessing multi-collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43254cbb-f912-44df-909d-df39f3b920d9",
   "metadata": {},
   "source": [
    "Categorical variables will now be one-hot encoded: We get dummies for our categorical variables to get the dataset ready for multicollinearity analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b6b99-2c3f-4c3c-ad35-06293cf0269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = pd.get_dummies(airbnb)\n",
    "transformed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae87799-385a-457e-9ff2-0c35a69e332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with nulls\n",
    "nulos=pd.DataFrame({'null_values':np.round(transformed_df.isnull().mean(), 2)})\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) #to show full dataframe\n",
    "print(nulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b437482-70b4-431e-8429-3ea1cad9c666",
   "metadata": {},
   "source": [
    "We can see that all the null values correspond to amenities, which means this amenity doesnt exist in the listing. so it means it is supposed to be a zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e91820-fd55-4f00-81dc-1984cc640840",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df['check_in_24h'] = transformed_df['check_in_24h'].fillna(0)\n",
    "transformed_df['air_conditioning'] = transformed_df['air_conditioning'].fillna(0)\n",
    "transformed_df['high_end_electronics'] = transformed_df['high_end_electronics'].fillna(0)\n",
    "transformed_df['bbq'] = transformed_df['bbq'].fillna(0)\n",
    "transformed_df['balcony'] = transformed_df['balcony'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8453a85-9bf6-4da6-afc2-e3d95d076071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_collinearity_heatmap(df, figsize=(11,9)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a heatmap of correlations between features in the df. A figure size can optionally be set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the style of the visualization\n",
    "    sns.set(style=\"white\")\n",
    "\n",
    "    # Create a covariance matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask the size of our covariance matrix\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, vmax=corr[corr != 1.0].max().max());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d62c73-93f3-4af6-a19e-cc82b22b976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_collinearity_heatmap(transformed_df, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06364ebb-e1df-4c3d-ab7d-bde08f4c38cb",
   "metadata": {},
   "source": [
    "It doesn't look like there are any significant collinear relationships with neighbourhood variables, so these will temporarily be dropped to produce a clearer heatmap for the remaining features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf0ed5-a989-41e2-9d8f-9b67942dee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_collinearity_heatmap(transformed_df.drop(list(transformed_df.columns[transformed_df.columns.str.startswith('neighborhood')]), axis=1), figsize=(25,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84231f23-f5e4-46b4-896a-c37ca9373c49",
   "metadata": {},
   "source": [
    "Areas of multi-collinearity:\n",
    "\n",
    "* Beds, bedrooms and the number of people that a property accommodates are highly correlated. The number of people accommodated has traditionally been a more high priority search parameter on Airbnb, as it is more relevant for private and shared rooms than the number of bedrooms (and is still the second highest priority parameter when searching on the site, after dates.\n",
    "\n",
    "* Unsurprisingly, there are perfect correlations between NaN reviews (i.e. listings that are not reviewed yet) for different review categories, and first and last review times. NaN categories can therefore be dropped.\n",
    "\n",
    "* The same is true of host_response_rate_unknown and host_response_time_unknown. One of these rates will be dropped.\n",
    "\n",
    "* There is a correlation between host_response_rate 0-49% and host_response_time_a few days or more. One of these will be dropped.\n",
    "\n",
    "* There are strong negative correlations between property_type_House and property_type_Apartment, and between room_type_Private room and room_type_Entire_home_apt (as these were the main two categories of their features before they were one-hot encoded). Although these are important categories, one of each will be dropped in order to reduce multi-collinearity (apartments and private rooms, as these are the second most common categories).\n",
    "* availabilty_30 and availability_365 are also positively high correlated. one of these will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60938d4-d8b5-4c0f-a079-5fcc07643f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping collinear features\n",
    "to_drop = ['beds',\n",
    "           'bedrooms',\n",
    "           'host_response_rate_unknown',\n",
    "           'host_response_rate_0-49%',\n",
    "           'property_type_Apartment',\n",
    "           'room_type_Private room',\n",
    "          'availability_30']\n",
    "to_drop.extend(list(transformed_df.columns[transformed_df.columns.str.endswith('nan')]))\n",
    "to_drop.extend(list(transformed_df.columns[transformed_df.columns.str.startswith('neighborhood')])) #no need to have latitude, longitude and neighboorhood, choose \n",
    "transformed_df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a04c6-9f7f-4fe7-872c-a9f1cbbd9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index as we deleted some rows \n",
    "transformed_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8dc3bc-b9ad-4ab9-a762-ef244d951dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final assessment of multi-collinearity\n",
    "\n",
    "\n",
    "multi_collinearity_heatmap(transformed_df.drop(list(transformed_df.columns[transformed_df.columns.str.startswith('neighborhood')]), axis=1), figsize=(25,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e868f-4588-46dd-bd58-34523b667516",
   "metadata": {},
   "source": [
    "There are still some fairly strong correlations between highly rated properties of different reviews categories - i.e. if a property gets a 10/10 for one category, it is likely to get a 10/10 for other categories. However, these will be left in for now and can be experimented with later to see if removing them improves the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c35751-d0b6-429f-8a56-612b61078ace",
   "metadata": {},
   "source": [
    "### Standardising and normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec7742-599e-4f3c-8a40-89638091e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['accommodates', 'availability_365', 'bathrooms',\n",
    "                      'host_days_active',\n",
    "                     'host_listings_count', 'maximum_nights', 'minimum_nights', \n",
    "                     'number_of_reviews', 'price','latitude','longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18da64f-3c45-452f-8d02-108e79675468",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df[numerical_columns].hist(figsize=(10,11));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72964435-7f25-4541-83b7-0c6a31a418e7",
   "metadata": {},
   "source": [
    "Other than availability_365 , latitude,longitude, accommodates, host_days_active, the remaining numerical features are all postively skewed and could benefit from log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfabad-0467-431b-85b7-e1fb3bffad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transforming columns\n",
    "numerical_columns = [i for i in numerical_columns \n",
    "                     if i not in ['availability_365', 'host_days_active','latitude','longitude','accommodates']] # Removing items not to be transformed\n",
    "\n",
    "for col in numerical_columns:\n",
    "    transformed_df[col] = transformed_df[col].astype('float64').replace(0.0, 0.01) # Replacing 0s with 0.01\n",
    "    transformed_df[col] = np.log(transformed_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ddf91-3d9e-4662-be91-0772f3e7ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df[numerical_columns].hist(figsize=(10,11));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274eaac-5b51-43f5-af28-7525d8da737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the price\n",
    "airbnb.price.hist(figsize=(15,5), bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989fdf5-9182-4125-a7ba-41ff48dd6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the number of days since first review\n",
    "transformed_df.price.hist(figsize=(15,5), bins=30);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb208d8-20c4-4ad0-bc39-417805779759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks much better than before \n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "def plotting_to_check_skewness():\n",
    "    for col in ['price']:\n",
    "        # to set the facecolor\n",
    "        plt.figure(dpi=500, facecolor = 'w')\n",
    "        # setting the limit on the x axis to be able to visualize as we have a big outliers\n",
    "        plt.xlim(0, 700)\n",
    "        \n",
    "        sns.distplot(airbnb[col], kde=True, bins='auto')\n",
    "        \n",
    "        # Remove the splines \n",
    "        plt.gca().spines[\"top\"].set_visible(False)\n",
    "        plt.gca().spines[\"bottom\"].set_visible(False)\n",
    "        plt.gca().spines[\"right\"].set_visible(False)\n",
    "        plt.gca().spines[\"left\"].set_visible(False)\n",
    "\n",
    "        plt.tight_layout() # Makes it better looking specially on laptops\n",
    "\n",
    "        # to save the fig\n",
    "        plt.savefig('skew.png',bbox_inches='tight', dpi=500, facecolor = '#dadada')\n",
    "        plt.title('Price distribution before log',fontsize=20)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "plotting_to_check_skewness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74b42b-205a-4c19-b8d7-71535eb4a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks much better than before \n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "def plotting_to_check_skewness():\n",
    "    for col in ['price']:\n",
    "        # to set the facecolor\n",
    "        plt.figure(dpi=500, facecolor = 'white')\n",
    "        # setting the limit on the x axis to be able to visualize as we have a big outliers\n",
    "        plt.xlim(3, 6)\n",
    "        \n",
    "        sns.distplot(transformed_df[col], kde=True, bins='auto')\n",
    "        \n",
    "        # Remove the splines \n",
    "        plt.gca().spines[\"top\"].set_visible(False)\n",
    "        plt.gca().spines[\"bottom\"].set_visible(False)\n",
    "        plt.gca().spines[\"right\"].set_visible(False)\n",
    "        plt.gca().spines[\"left\"].set_visible(False)\n",
    "\n",
    "        plt.tight_layout() # Makes it better looking specially on laptops\n",
    "\n",
    "        # to save the fig\n",
    "        plt.savefig('skew.png',bbox_inches='tight', dpi=500, facecolor = '#dadada')\n",
    "        #plt.title('Price distribution after log transformation', fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "plotting_to_check_skewness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3705eb-8613-48ca-af0c-aea8b56a2f8f",
   "metadata": {},
   "source": [
    "This appears to have helped some of the distributions, although some (e.g. number_of_reviews, minimum_nights ) contain a large number of 0s, which means these features are not normally distributed. Most importantly, however, the target variable price now appears much more normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbcd5d-3fdc-48d3-8b15-366ae0b4cfa6",
   "metadata": {},
   "source": [
    "-> Finally, the predictive features X and the target feature y can be separated, and X will be scaled. StandardScaler from sklearn will be used, but the type of scaling used could be experimented with later to see if alternative versions yield better results.\n",
    "\n",
    "That is, we’ll separate the features and the target variable for modeling. We will assign the features (explanatory variables) to X and the target variable to y. We use scaler.fit_transform(), as mentioned above, to transform the y variable for the model. transformed_df.drop([features], axis=1) tells pandas which columns we want to exclude. We won’t include price for obvious reasons, and ID is just an index with no relationship to price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dff5f-dfc2-4900-8c63-072770497152",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = transformed_df[np.isfinite(transformed_df).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678831c7-82f0-4e9e-b35f-0a61889c888d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc924233-d7e2-4f21-87bc-e356e2b8159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X = transformed_df.drop('price', axis=1)\n",
    "y = transformed_df.price\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4250eb-8170-40e0-9794-3ea94ccbc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9cbe0-42d6-42cc-acc2-68ce2cbd29f3",
   "metadata": {},
   "source": [
    "# 2. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b4bd9-944e-4916-8a52-435d649c7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039e17b-67c5-40ed-b517-fca5da672d8a",
   "metadata": {},
   "source": [
    "Now that the data preprocessing is over, we can start applying different Supervised Machine Learning models. I will compare two models:\n",
    "\n",
    "* A Spatial Hedonic Price Model (OLS Regression), with the LinearRegression from Scikit-Learn library and regression tree- **explainable models**\n",
    "* The Gradient Boosting method, with the XGBRegressor from the XGBoost library- **black-box models**\n",
    "\n",
    "The evaluation metrics used will be mean squared error (for loss) and r-squared (for accuracy).For evaluation metrics i will be using Mean Absolute Error as it is not affected by outliers unlike Mean Squared Error. **choose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e35c00-427d-43a0-930d-03786908af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eeb527-e77c-4bde-b86e-5267014fc9a1",
   "metadata": {},
   "source": [
    "### Model 1: Spatial Hedonic Price Model (HPM)- Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dfa878-1d1a-493c-9918-e9429546f45f",
   "metadata": {},
   "source": [
    "The hedonic model involves regressing observed asking-prices for the listing against those attributes of a property hypothesized to be determinants of the asking-price. It comes from hedonic price theory which assumes that a commodity, such as a house can be viewed as an aggregation of individual components or attributes (Griliches, 1971). Consumers are assumed to purchase goods embodying bundles of attributes that maximize their underlying utility functions (Rosen, 1974).\n",
    "\n",
    "In addition to the characteristics of the Airbnb listings, we add location features as they have been shown to be important factors in influencing the price . Ideally, Lagrange multiplier tests should be conducted to verify if there is spatial lag in the dependent variable and therefore a spatial lag model is preferred for estimating a spatial HPM. However, for the purposes of this thesis, I am only using a conventional OLS model for hedonic price estimation that includes spatial and locational features, but not a spatial lag that accounts for spatial dependence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25296b-4058-4554-8f0a-1876c5b7d32f",
   "metadata": {},
   "source": [
    "### https://github.com/gracecarrillo/Predicting-Airbnb-prices-with-machine-learning-and-location-data/blob/gh-pages/Exploring_Edinburgh_Graciela_Carrillo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ca43b-5707-4bc3-8c2d-6ea09aac0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "hpm_reg = LinearRegression()  \n",
    "hpm_reg.fit(X_train, y_train) #training the algorithm\n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "training_preds_hpm_reg = hpm_reg.predict(X_train)\n",
    "val_preds_hpm_reg = hpm_reg.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"\\nTraining RMSE:\", round(mean_squared_error(y_train, training_preds_hpm_reg),4))\n",
    "print(\"Validation RMSE:\", round(mean_squared_error(y_test, val_preds_hpm_reg),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_hpm_reg),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_hpm_reg),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7feab-9c6f-45c9-997d-a873e840c5c8",
   "metadata": {},
   "source": [
    "This means that our features explain approximately 38% of the variance in our target variable.\n",
    "\n",
    "Interpreting the mean_squared_error value is somewhat more intuitive that the r-squared value. The RMSE measures the distance between our predicted values and actual values.\n",
    "\n",
    "We can compare the actual output values for X_test with the predicted values graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcac90a-0dfc-4448-8494-205b0512d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = y_test\n",
    "plt.scatter(val_preds_hpm_reg, actual_values, alpha=.7,\n",
    "            color='r') #alpha helps to show overlapping data\n",
    "overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n",
    "                    (round(r2_score(y_test, val_preds_hpm_reg),4)),\n",
    "                    (round(mean_squared_error(y_test, val_preds_hpm_reg))),4)\n",
    "plt.annotate( s=overlay,xy=(5.5,2.5),size='x-large')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('linear Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62867c-c69b-4c6e-89c7-49c871af6d06",
   "metadata": {},
   "source": [
    "### Ridge Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fdad29-8f9c-4cfe-8919-2b43e7d14178",
   "metadata": {},
   "source": [
    "We can try using Ridge Regularization to decrease the influence of less important features. Ridge Regularization is a process which shrinks the regression coefficients of less important features.\n",
    "\n",
    "We’ll once again instantiate the model. The Ridge Regularization model takes a parameter, alpha , which controls the strength of the regularization.\n",
    "\n",
    "We’ll experiment by looping through a few different values of alpha, and see how this changes our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633f3fe-a87c-44ca-bbfd-621cde5dcabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "for i in range (-2, 3):\n",
    "    alpha = 10**i\n",
    "    rm = linear_model.Ridge(alpha=alpha)\n",
    "    ridge_model = rm.fit(X_train, y_train)\n",
    "    preds_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "    plt.scatter(preds_ridge, actual_values, alpha=.75, color='r')\n",
    "    plt.xlabel('Predicted Price')\n",
    "    plt.ylabel('Actual Price')\n",
    "    plt.title('Ridge Regularization with alpha = {}'.format(alpha))\n",
    "    overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n",
    "                   round(ridge_model.score(X_test, y_test), 4),\n",
    "                    round(mean_squared_error(y_train, training_preds_hpm_reg),4))\n",
    "    plt.annotate( s=overlay,xy=(5.5,2.5),size='x-large')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc1c5f-244f-4f57-8549-0de194231a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7637fc-dd61-47f1-8c06-35efbbffd974",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "# instanciando os modelos\n",
    "dtr = DecisionTreeRegressor()\n",
    "rfr = RandomForestRegressor(n_estimators=1000)\n",
    "svr = SVR()\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "gbr = GradientBoostingRegressor(random_state=seed)\n",
    "xgb= xgb.XGBRegressor()\n",
    "\n",
    "# criando uma lista de tuplas com o nome e modelo treinado\n",
    "models = [('RIDGE', ridge),\n",
    "          ('LASSO', lasso),\n",
    "          ('GRADIENT BOOSTING', gbr),\n",
    "          ('XGboost', xgb)\n",
    "          ]\n",
    "\n",
    "# rodando o loop para obter os resultados\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_test)\n",
    "    \n",
    "    print(f'{name}')\n",
    "    print(f'Previsão on training data:')\n",
    "    print('-----------------------------------------------------')\n",
    "    print(f'Mean Squared Error: {mean_squared_error(y_train, y_pred_train)}')\n",
    "    print(f'Mean Absolute Error: {mean_absolute_error(y_train, y_pred_train)}')\n",
    "    print(f'R2 score: {r2_score(y_train, y_pred_train)}\\n')\n",
    "    \n",
    "    print(f'Previsão on test data:')\n",
    "    print('-----------------------------------------------------')\n",
    "    print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred_val)}')\n",
    "    print(f'Mean Absolute Error: {mean_absolute_error(y_test, y_pred_val)}')\n",
    "    print(f'R2 score: {r2_score(y_test, y_pred_val)}')\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaf7ef-41e9-4cf7-8611-aa0f62fbdf19",
   "metadata": {},
   "source": [
    "As we can see, it substantially improved our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4a7cf-d5b6-4483-ad77-7cab928e9bf0",
   "metadata": {},
   "source": [
    "### Model 2: Gradient boosted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a8c28-ce01-4717-a20f-585aba7d3fa2",
   "metadata": {},
   "source": [
    "Boosting is an ensemble technique where new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made. A popular example is the AdaBoost algorithm that weights data points that are hard to predict.\n",
    "\n",
    "Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b1c9d-4143-4721-9a93-b2b6074b3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "gbr_reg = GradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "#xgb_reg = xgb.XGBRegressor()\n",
    "gbr_reg.fit(X_train, y_train)\n",
    "training_preds_gbr_reg = gbr_reg.predict(X_train)\n",
    "val_preds_gbr_reg = gbr_reg.predict(X_test)\n",
    "\n",
    "print(\"\\nTraining MSE:\", round(mean_squared_error(y_train, training_preds_gbr_reg),4))\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_preds_gbr_reg),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_gbr_reg),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_gbr_reg),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4f633-406c-432c-9a2d-c8d86af1c774",
   "metadata": {},
   "source": [
    "This means that our features explain approximately 50% of the variance in our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43be0a-1c5e-4b9d-9ac6-8a581c38c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = y_test\n",
    "plt.scatter(val_preds_gbr_reg, actual_values, alpha=.7,\n",
    "            color='r') #alpha helps to show overlapping data\n",
    "overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n",
    "                    (round(r2_score(y_test, val_preds_gbr_reg),4)),\n",
    "                    (round(mean_squared_error(y_test, val_preds_gbr_reg))),4)\n",
    "plt.annotate( s=overlay,xy=(5.5,2.5),size='x-large')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('GradientBoosting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8a0e5-ae6d-4206-a478-3eef88f16039",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281f0da-e5db-4838-a12b-c2230b1fa367",
   "metadata": {},
   "source": [
    "Apart from its superior performance, a benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model.\n",
    "\n",
    "Generally, importance provides a score that indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance.\n",
    "\n",
    "This importance is calculated explicitly for each attribute in the dataset, allowing attributes to be ranked and compared to each other.\n",
    "\n",
    "Importance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity (Gini index) used to select the split points or another more specific error function.\n",
    "\n",
    "The feature importances are then averaged across all of the the decision trees within the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31403c76-1760-4110-ab3f-78dc2066077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_gbr_reg = pd.DataFrame(gbr_reg.feature_importances_, columns=['weight'], index=X_train.columns)\n",
    "ft_weights_gbr_reg.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_gbr_reg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a7787-5193-4776-951a-3da42f82b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_xgb_reg = pd.DataFrame(xgb.feature_importances_, columns=['weight'], index=X_train.columns)\n",
    "ft_weights_xgb_reg.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_xgb_reg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4756352-c3af-422c-91fd-aae74b4fc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importances\n",
    "plt.figure(figsize=(10,25))\n",
    "plt.barh(ft_weights_gbr_reg.index, ft_weights_gbr_reg.weight, align='center') \n",
    "plt.title(\"Feature importances in the Gradient Boosted Trees model\", fontsize=14)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.margins(y=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675eb2ce-0478-4eca-bbb4-85529ce5c90d",
   "metadata": {},
   "source": [
    "About a good number of features have a feature importance of 0 in this Gradient Boosting regression model, and could potentially be removed.\n",
    "\n",
    "The top 10 most important features are:\n",
    "\n",
    "* 1-How many people the property accommodates (accommodates)\n",
    "* 2-The number of bathrooms (bathrooms)\n",
    "* 3-The number of reviews (number_of_reviews)\n",
    "* 4-AC\n",
    "* 5-If the rental is the entire flat or not room_type_Entire home/apt\n",
    "* 6-Hot tub or sauna\n",
    "* 7-LAtitude\n",
    "* 8- lONGITUDE\n",
    "* 9-How many days are available to book out of the next 365 (availability_365)\n",
    "* 10- Host days active \n",
    "\n",
    "The most important features is How many people the property accommodates. Which makes sense. Asking price is higher if the offer is it can accomodate more people. This could also suggest that offering space for more people , may be better overall, given the large difference in importance compared to the second most important feature (half the importance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35488656-7b30-4943-9846-0a5445596a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median price for different host listing counts\n",
    "plt.figure()\n",
    "transformed_df.groupby('host_listings_count').price.median().plot(figsize=(20,4), kind='bar')\n",
    "plt.title('Median price of listings hosted by hosts who are responsible for other properties')\n",
    "plt.xlabel('Number of properties managed by hosts')\n",
    "plt.ylabel('Median price (€)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132c443-402f-4a43-a138-618b1d341b43",
   "metadata": {},
   "source": [
    "### Improving models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074e4bc7-bb3f-4526-90e6-03e033dd4432",
   "metadata": {},
   "source": [
    "In the 'Preparing the data for modeling' section above, it was noted that a lot of the review columns are reasonably highly correlated with each other. They were left in to see whether they would be useful after all. However, the feature importances graph produced by the XGBoost model suggest that they were of relatively low importance. Also 'time_since' variables dont seem important\n",
    "\n",
    "This model will drop review columns other than the overall review rating, and use the same Hedonic regression and Gradient Boosting structure, in order to see whether this produces a better models.\n",
    "\n",
    "Columns will be dropped from the existing X_train and X_test split, for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3a6e0-e0be-4bfa-93c6-d90bc25825a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review = list(X_train.columns[X_train.columns.str.startswith(\"review_scores\")])\n",
    "review_to_keep = list(X_train.columns[X_train.columns.str.startswith(\"review_scores_rating\")])\n",
    "all_times_since=list(X_train.columns[X_train.columns.str.startswith(\"time_since\")])\n",
    "review_to_drop = [x for x in all_review if x not in review_to_keep]\n",
    "\n",
    "X_train_short = X_train.drop(review_to_drop, axis=1)\n",
    "X_test_short = X_test.drop(review_to_drop, axis=1)\n",
    "X_train_short = X_train_short.drop(all_times_since, axis=1)\n",
    "X_test_short = X_test_short.drop(all_times_since, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642dddd-d2b9-40f1-b903-f1b73b0e6bc4",
   "metadata": {},
   "source": [
    "### Model 3: Hedonic regression with dropped columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34078e-3e9b-4fd1-b268-2ca3a2a211cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "hpm_reg2 = LinearRegression()  \n",
    "hpm_reg2.fit(X_train_short, y_train) #training the algorithm\n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "training_preds_hpm_reg2 = hpm_reg2.predict(X_train_short)\n",
    "val_preds_hpm_reg2 = hpm_reg2.predict(X_test_short)\n",
    "\n",
    "\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train, training_preds_hpm_reg2),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test, val_preds_hpm_reg2),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_hpm_reg2),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_hpm_reg2),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e76b2a-6253-4bdd-8087-b8bbf5fd80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test, val_preds_hpm_reg2)) * (len(y_test)-1)/(len(y_test)-X_test_short.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8bb465-60a8-46fe-bbfc-76acd6b89551",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "cdf1 = pd.DataFrame(hpm_reg2.coef_, X_train_short.columns, columns=['Coefficients'])\n",
    "print(cdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530ebc8-3869-4a91-8a68-b7e2d3de21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "cdf1['Coefficients'] = cdf1.apply(lambda row: np.exp(row['Coefficients'])-1, axis=1) #fazer ifelse para as colunas q tao em log e falta a significancia dos coeficientes\n",
    "cdf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901bfe8-a7e5-40a6-87a7-9201c42fa042",
   "metadata": {},
   "source": [
    "## Checking LR assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0309ce6-c333-4920-8b06-138fd0f75626",
   "metadata": {},
   "source": [
    "**Linearity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2486c-bb43-4598-b6e7-66a1340adf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = y_test\n",
    "plt.scatter(val_preds_hpm_reg2, actual_values, alpha=.7,\n",
    "            color='r') #alpha helps to show overlapping data\n",
    "overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n",
    "                    (round(r2_score(y_test, val_preds_hpm_reg2),4)),\n",
    "                    (round(mean_squared_error(y_test, val_preds_hpm_reg2))),4)\n",
    "plt.annotate( s=overlay,xy=(5.5,2.5),size='x-large')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('linear Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb938f98-e8d2-4de6-870a-9768695bc3d0",
   "metadata": {},
   "source": [
    "**Normality of the Error Terms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d96a33-bda2-4916-b7f7-eebce2e6cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_residuals(model, features, label):\n",
    "    \"\"\"\n",
    "    Creates predictions on the features with the model and calculates residuals\n",
    "    \"\"\"\n",
    "    predictions = model.predict(features)\n",
    "    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
    "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f637e-325b-440f-9bf6-3aebbb4316ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_errors_assumption(model, features, label, p_value_thresh=0.05):\n",
    "    \"\"\"\n",
    "    Normality: Assumes that the error terms are normally distributed. If they are not,\n",
    "    nonlinear transformations of variables may solve this.\n",
    "               \n",
    "    This assumption being violated primarily causes issues with the confidence intervals\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.diagnostic import normal_ad\n",
    "    print('Assumption 2: The error terms are normally distributed', '\\n')\n",
    "    \n",
    "    # Calculating residuals for the Anderson-Darling test\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "    \n",
    "    print('Using the Anderson-Darling test for normal distribution')\n",
    "\n",
    "    # Performing the test on the residuals\n",
    "    p_value = normal_ad(df_results['Residuals'])[1]\n",
    "    print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
    "    \n",
    "    # Reporting the normality of the residuals\n",
    "    if p_value < p_value_thresh:\n",
    "        print('Residuals are not normally distributed')\n",
    "    else:\n",
    "        print('Residuals are normally distributed')\n",
    "    \n",
    "    # Plotting the residuals distribution\n",
    "    plt.subplots(figsize=(12, 6))\n",
    "    plt.title('Distribution of Residuals')\n",
    "    sns.distplot(df_results['Residuals'])\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    if p_value > p_value_thresh:\n",
    "        print('Assumption satisfied')\n",
    "    else:\n",
    "        print('Assumption not satisfied')\n",
    "        print()\n",
    "        print('Confidence intervals will likely be affected')\n",
    "        print('Try performing nonlinear transformations on variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef947d-4cf5-4cc0-b48e-48787192da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_errors_assumption(hpm_reg2, X_train_short, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c06151-63b6-471a-a819-8fab2bcf8437",
   "metadata": {},
   "source": [
    "**No Multicollinearity among Predictors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b674c0-6019-4ec0-8aed-b79745c6d779",
   "metadata": {},
   "source": [
    "Already checked above the preparing data for modelling section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994b862-1a96-4b68-842c-e00a37fcca7f",
   "metadata": {},
   "source": [
    "**No Autocorrelation of the Error Terms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64062a3-643c-41a9-b9b6-eb72f32ecb72",
   "metadata": {},
   "source": [
    "This assumes no autocorrelation of the error terms. Autocorrelation being present typically indicates that we are missing some information that should be captured by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44653741-02b7-4a0f-9c3a-ad369b527c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n",
    "                     autocorrelation, then there is a pattern that is not explained due to\n",
    "                     the current value being dependent on the previous value.\n",
    "                     This may be resolved by adding a lag variable of either the dependent\n",
    "                     variable or some of the predictors.\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.stattools import durbin_watson\n",
    "    print('Assumption 4: No Autocorrelation', '\\n')\n",
    "    \n",
    "    # Calculating residuals for the Durbin Watson-tests\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "\n",
    "    print('\\nPerforming Durbin-Watson Test')\n",
    "    print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
    "    print('0 to 2< is positive autocorrelation')\n",
    "    print('>2 to 4 is negative autocorrelation')\n",
    "    print('-------------------------------------')\n",
    "    durbinWatson = durbin_watson(df_results['Residuals'])\n",
    "    print('Durbin-Watson:', durbinWatson)\n",
    "    if durbinWatson < 1.5:\n",
    "        print('Signs of positive autocorrelation', '\\n')\n",
    "        print('Assumption not satisfied')\n",
    "    elif durbinWatson > 2.5:\n",
    "        print('Signs of negative autocorrelation', '\\n')\n",
    "        print('Assumption not satisfied')\n",
    "    else:\n",
    "        print('Little to no autocorrelation', '\\n')\n",
    "        print('Assumption satisfied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d449db6-680e-409e-90d8-f227d6f24c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_assumption(hpm_reg2, X_train_short, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c5143-373e-43b1-8dbc-539d93787057",
   "metadata": {},
   "source": [
    "**Homoscedasticity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcec48-2be3-44ca-9e49-2b3c0de96881",
   "metadata": {},
   "source": [
    "This assumes homoscedasticity, which is the same variance within our error terms. Heteroscedasticity, the violation of homoscedasticity, occurs when we don’t have an even variance across the error terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0d13b-728e-433a-ab18-48a622956149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homoscedasticity_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Homoscedasticity: Assumes that the errors exhibit constant variance\n",
    "    \"\"\"\n",
    "    print('Assumption 5: Homoscedasticity of Error Terms', '\\n')\n",
    "    \n",
    "    print('Residuals should have relative constant variance')\n",
    "        \n",
    "    # Calculating residuals for the plot\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "\n",
    "    # Plotting the residuals\n",
    "    plt.subplots(figsize=(12, 6))\n",
    "    ax = plt.subplot(111)  # To remove spines\n",
    "    plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n",
    "    plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n",
    "    ax.spines['right'].set_visible(False)  # Removing the right spine\n",
    "    ax.spines['top'].set_visible(False)  # Removing the top spine\n",
    "    plt.title('Residuals')\n",
    "    plt.show() \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d738ad-1666-4ad6-b5ff-f20d1c44d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "homoscedasticity_assumption(hpm_reg2, X_train_short, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7dda1-40b2-44db-8adc-df3bcdd085bb",
   "metadata": {},
   "source": [
    "There seems no be no problem with this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4af381-0f83-42ab-809a-0402eb41de82",
   "metadata": {},
   "source": [
    "### Model 4: Gradient Boosting with dropped columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a51e0-68b5-4e91-a5e9-2d1d67ec1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_reg2 = GradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "\n",
    "gbr_reg2.fit(X_train_short, y_train)\n",
    "training_preds_gbr_reg2 = gbr_reg2.predict(X_train_short)\n",
    "val_preds_gbr_reg2 = gbr_reg2.predict(X_test_short)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train, training_preds_gbr_reg2),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test, val_preds_gbr_reg2),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_gbr_reg2),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_gbr_reg2),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a018827-afee-4438-85f1-e023cae22b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test, val_preds_gbr_reg2)) * (len(y_test)-1)/(len(y_test)-X_test_short.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c11e1-c9a8-410a-ab61-6e49cf411ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_gbr_reg2 = pd.DataFrame(gbr_reg2.feature_importances_, columns=['weight'], index=X_train_short.columns)\n",
    "ft_weights_gbr_reg2.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_gbr_reg2.head(10)\n",
    "#ft_weights_gbr_reg2.to_csv('feature_importance1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02459e0a-f143-40d8-8ca3-930198d16aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importances\n",
    "plt.figure(figsize=(10,25))\n",
    "plt.barh(ft_weights_gbr_reg2.index, ft_weights_gbr_reg2.weight, align='center') \n",
    "plt.title(\"Feature importances in the Gradient Boosted Trees model\", fontsize=14)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.margins(y=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b80341-0c37-4130-bc2a-a1e7a3d052dd",
   "metadata": {},
   "source": [
    " Spatial Hedonic Regression improves its performance a lot, specially, the difference in performance between train and test data and GBT performs almost exactly the same without the additional review columns.\n",
    "\n",
    "Hence, because they are able to achieve the same performance with 18 fewer columns, the second models are the preferred models as they require less data and are less computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9bad4-f1b8-446d-bbc9-92099ba79e08",
   "metadata": {},
   "source": [
    "By eliminating the *time_since* variables that were of 0 importance we were able to slighlty improve our model, at the same time we reduced the complexity. Thus, this shorten dataset must be prefered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48582023-83b1-4591-8ebc-cbc7e009836a",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4592de-4f31-43bd-87a1-2fc2acc4f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test_short.columns\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(gbr_reg2)  # model used \n",
    "\n",
    "shap_values = explainer.shap_values(X_test_short.iloc[700]) # predicting 50 row of the test dataset\n",
    "shap.initjs()\n",
    "\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=shap_values,\n",
    "    features=features\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0ebd6-17fa-4754-83b5-d43dc387bf24",
   "metadata": {},
   "source": [
    "The force plot is another way to see the effect each feature has on the prediction, for a given observation. In this plot the positive SHAP values are displayed on the left side and the negative on the right side, as if competing against each other. Features in red color influence positiv, features in blue color - the opposite. The base value is the average of all output values of the model on the training The highlighted value is the prediction for that observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca6d0e-d101-4237-ae44-dd23fc110807",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(gbr_reg2)  # model used \n",
    "\n",
    "shap_values = explainer(X_test_short) \n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f19526-cbdc-4ed3-a587-8aee5c8c5bf5",
   "metadata": {},
   "source": [
    "On the beeswarm the features are also ordered by their effect on prediction, but we can also see how higher and lower values of the feature will affect the result.\n",
    "\n",
    "All the little dots on the plot represent a single observation. The horizontal axis represents the SHAP value, while the color of the point shows us if that observation has a higher or a lower value, when compared to other observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c02248-6f29-49d0-b02e-79d0dee41d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a20f3-f5a8-4ed0-990c-615510352e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fbbc2-ef16-4878-aca4-1690c4ffa9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get expected value and shap values array\n",
    "expected_value = explainer.expected_value\n",
    "shap_array = explainer.shap_values(X_test_short)\n",
    "\n",
    "#Descion plot for first 10 observations\n",
    "shap.decision_plot(expected_value, shap_array[0:10],feature_names=list(X_test_short.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2bfa7f-40ad-4348-953b-01ae61886516",
   "metadata": {},
   "source": [
    "### Enriching the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e52596-c765-412c-b282-2a96eb3ded92",
   "metadata": {},
   "source": [
    "**distance to the airport**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5f8e6-a42a-42ec-9e64-361c6e112d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy\n",
    "from geopy import distance\n",
    "\n",
    "#transformed_df['dist_aeroporto'] = 0\n",
    "\n",
    "def distance_2points(row):\n",
    "    aeroporto = (38.773226, -9.134244)\n",
    "    coords = (row['latitude'], row['longitude'])\n",
    "    results = geopy.distance.geodesic(aeroporto, coords).kilometers\n",
    "    return results\n",
    "\n",
    "#airbnb['dist_aeroporto'] = airbnb.apply(distance_2points, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ea54d-c9e7-42fc-8b02-e6726280c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformed_df['dist_aeroporto'] = transformed_df.apply(lambda row: distance_2points(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4679275-a424-451e-ac98-e73df6cb81ef",
   "metadata": {},
   "source": [
    "**distance to the closest attraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfbbbc-a7f9-476f-9ddc-c08f04d18e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "castelo_s_jorge=(38.71385,-9.133545)\n",
    "terreiro_paco=(38.707146,-9.136148)\n",
    "torre_belem=(38.691547,-9.215905)\n",
    "jeronimos=(38.697809,-9.206761)\n",
    "padrao_descobrimentos=(38.693521,-9.205660)\n",
    "time_out=(38.706982,-9.145583)\n",
    "miradouro_graca=(38.716306,-9.131507)\n",
    "\n",
    "#transformed_df['dist_nearist_attraction'] = 0\n",
    "\n",
    "def distance_2points_attraction(row):\n",
    "    coords = (row['latitude'], row['longitude'])\n",
    "    dist_castelo_s_jorge = geopy.distance.geodesic(castelo_s_jorge, coords).kilometers\n",
    "    dist_terreiro_paco = geopy.distance.geodesic(terreiro_paco, coords).kilometers\n",
    "    dist_torre_belem = geopy.distance.geodesic(torre_belem, coords).kilometers\n",
    "    dist_jeronimos = geopy.distance.geodesic(jeronimos, coords).kilometers\n",
    "    dist_padrao_descobrimentos = geopy.distance.geodesic(padrao_descobrimentos, coords).kilometers\n",
    "    dist_time_out = geopy.distance.geodesic(time_out, coords).kilometers\n",
    "    dist_miradouro_graca = geopy.distance.geodesic(miradouro_graca, coords).kilometers\n",
    "    min_dist = min(dist_castelo_s_jorge, dist_terreiro_paco, dist_torre_belem, dist_jeronimos, dist_padrao_descobrimentos, dist_time_out, dist_miradouro_graca)\n",
    "    return min_dist\n",
    "\n",
    "#airbnb['dist_aeroporto'] = airbnb.apply(distance_2points, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e338fc7-0762-4926-bdba-f95f4646c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformed_df['dist_nearist_attraction'] = transformed_df.apply(lambda row: distance_2points_attraction(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238b2b4-416a-489b-99e8-e1788c77d1ae",
   "metadata": {},
   "source": [
    "**Google Maps API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1713a0-4528-4f5f-a8ca-6e54c48d5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9566ea-99d5-4649-9d28-1b314b036e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY= 'AIzaSyDBLkQJ0H-kGL_q7aBvzyQnNhb_Jq59Qxw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ca649-a358-406e-a1b1-d92d7cc174b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_client=googlemaps.Client(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de728a6a-3152-49a1-a261-2a3af8854fb5",
   "metadata": {},
   "source": [
    "**atms whithin 1km**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dc6dd-4f6c-416a-8d9f-f203d322b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_list=[]\n",
    "\n",
    "def getGooglePlaceData(row):\n",
    "        \n",
    "        \n",
    "            atm=map_client.places_nearby(\n",
    "                location=(row['latitude'],row['longitude']),\n",
    "                keyword='atm',\n",
    "                radius=1000,\n",
    "                type= 'atm')\n",
    "            \n",
    "            #df_listing['atm_no']=0\n",
    "            atm_list.extend(atm.get('results'))\n",
    "            \n",
    "            results = len(atm_list)\n",
    "\n",
    "            return results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45227b5b-6679-40b8-b274-ef73dc7dd66b",
   "metadata": {},
   "source": [
    "transformed_df['atm_no'] = transformed_df.apply(lambda row: getGooglePlaceData(row), axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c761ad29-8f31-4d79-b114-d7c7bc8b0ce7",
   "metadata": {},
   "source": [
    "transformed_df.to_csv('data/extended_dataATM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85823b-1e80-45b6-93d3-f3a36b64d1f8",
   "metadata": {},
   "source": [
    "**Bars and discos whithin 1km**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923efa2-5066-4490-9d2d-f518c7647653",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_list=[]\n",
    "def getGooglePlaceDatabarsanddisco(row):\n",
    "        \n",
    "        \n",
    "            bars=map_client.places_nearby(\n",
    "                location=(row['latitude'],row['longitude']),\n",
    "                keyword='discoteca',\n",
    "                radius=1000,\n",
    "                type= 'night_club' and 'bar')\n",
    "\n",
    "            \n",
    "            bars_list.extend(bars.get('results'))\n",
    "            \n",
    "            results = len(bars_list)\n",
    "\n",
    "            return results\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "df8bff4b-33c6-416d-a69c-4ca583113db2",
   "metadata": {},
   "source": [
    "\n",
    "transformed_df['bars_and_discos_no'] = transformed_df.apply(lambda row: getGooglePlaceDatabarsanddisco(row), axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "629cb53a-4c5e-41b3-a1df-5cb0d8b86b9b",
   "metadata": {},
   "source": [
    "transformed_df.to_csv('data/extended_dataATMandBars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0bac2-f170-4912-b559-747181b0b2ec",
   "metadata": {},
   "source": [
    "**Metros whithin 1km**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6eb07-fea8-4f5f-a587-e18da90ca012",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_and_discos_nometro_list=[]\n",
    "\n",
    "def getGooglePlaceDatametro(row):\n",
    "        \n",
    "        \n",
    "            metro=map_client.places_nearby(\n",
    "                location=(row['latitude'],row['longitude']),\n",
    "                keyword='metro',\n",
    "                radius=1000,\n",
    "                type= 'subway_station')\n",
    "            \n",
    "            \n",
    "            metro_list.extend(metro.get('results'))\n",
    "            \n",
    "            results = len(metro_list)\n",
    "\n",
    "            return results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5769d1ea-1318-4720-b4eb-4f33c7c01aa0",
   "metadata": {},
   "source": [
    "transformed_df['metro_no'] = transformed_df.apply(lambda row: getGooglePlaceDatametro(row), axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5eaece2c-65fd-4c63-9629-34577dc0fcdc",
   "metadata": {},
   "source": [
    "transformed_df.to_csv('data/extended_dataFinal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1fc44-36e2-4f71-bf0d-e1fbf2c65e74",
   "metadata": {},
   "source": [
    "**Read extended file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd0e63-bd93-4e9f-a963-c6a15a3c5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df2 =pd.read_csv(\"C:/Users/Madalena Nunes/OneDrive/Ambiente de Trabalho/Business Analytics/Tese/data/extended_dataFinal.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1ae7e-b5f1-4b01-8c0a-9873906cd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e154718-b6aa-4d35-9758-f058703ab496",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add= transformed_df2[['dist_aeroporto','dist_nearist_attraction','atm_no','bars_and_discos_no','metro_no']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c459e-2c55-4e07-b2b9-d54804a22686",
   "metadata": {},
   "source": [
    "### Further analysis of new variables contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4cf56-82f4-4f31-88ab-fdd7efb1104b",
   "metadata": {},
   "source": [
    "* **airbnb+ dist_nearist_attraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b8f36-4ca4-4523-907a-db480bb08c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2cols=transformed_df2['dist_nearist_attraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea3593-8b13-4f9d-b9c6-29e8076d10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2df=transformed_df.join(exp2cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b6545-cf58-4257-b8d7-241834ae82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X_exp2 = exp2df.drop('price', axis=1)\n",
    "y = exp2df.price\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_exp2 = pd.DataFrame(scaler.fit_transform(X_exp2), columns=list(X_exp2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a0498-7789-49fe-b71c-3579111d51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train_exp2, X_test_exp2, y_train2, y_test2 = train_test_split(X_exp2, y, test_size=0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99523ba-152d-48e7-9fa3-75137c629295",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review = list(X_train_exp2.columns[X_train_exp2.columns.str.startswith(\"review_scores\")])\n",
    "review_to_keep = list(X_train_exp2.columns[X_train_exp2.columns.str.startswith(\"review_scores_rating\")])\n",
    "review_to_drop = [x for x in all_review if x not in review_to_keep]\n",
    "all_times_since=list(X_train_exp2.columns[X_train_exp2.columns.str.startswith(\"time_since\")])\n",
    "\n",
    "\n",
    "X_train_exp2 = X_train_exp2.drop(review_to_drop, axis=1)\n",
    "X_train_exp2 = X_train_exp2.drop(all_times_since, axis=1)\n",
    "X_test_exp2 = X_test_exp2.drop(review_to_drop, axis=1)\n",
    "X_test_exp2 = X_test_exp2.drop(all_times_since, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff481a1e-4de9-4a83-bf5d-5cb326a9bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "gbr_regExp2= GradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "#xgb_reg = xgb.XGBRegressor()\n",
    "gbr_regExp2.fit(X_train_exp2, y_train2)\n",
    "training_preds_gbr_regExp2 = gbr_regExp2.predict(X_train_exp2)\n",
    "val_preds_gbr_regExp2 = gbr_regExp2.predict(X_test_exp2)\n",
    "\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train2, training_preds_gbr_regExp2),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test2, val_preds_gbr_regExp2),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train2, training_preds_gbr_regExp2),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test2, val_preds_gbr_regExp2),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956d795-fca1-4fb3-af23-1b25d7c4389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test2, val_preds_gbr_regExp2)) * (len(y_test2)-1)/(len(y_test2)-X_test_exp2.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ccd92-51ce-49e8-9960-12fbe02b23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_gbr_exp2 = pd.DataFrame(gbr_regExp2.feature_importances_, columns=['weight'], index=X_train_exp2.columns)\n",
    "ft_weights_gbr_exp2.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_gbr_exp2.head(10)\n",
    "#ft_weights_gbr_reg3.head(10).to_csv('feature_importance2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd88c0-8108-4113-b558-3c7b26cac48f",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecf27b-ee4c-486c-991b-7162f61d482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "hpm_regExp2 = LinearRegression()  \n",
    "hpm_regExp2.fit(X_train_exp2, y_train2) #training the algorithm\n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "training_preds_hpm_regExp2 = hpm_regExp2.predict(X_train_exp2)\n",
    "val_preds_hpm_regExp2 = hpm_regExp2.predict(X_test_exp2)\n",
    "\n",
    "\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train2, training_preds_hpm_regExp2),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test2, val_preds_hpm_regExp2),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train2, training_preds_hpm_regExp2),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test2, val_preds_hpm_regExp2),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2c771-11f5-4005-8e7a-789778d6bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test2, val_preds_hpm_regExp2)) * (len(y_test2)-1)/(len(y_test2)-X_test_exp2.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7feb537-ec00-4f8f-87c6-f9bffb1e261e",
   "metadata": {},
   "source": [
    "* **airbnb+ dist_nearist_attraction + dist_aeroporto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8da46e-bcbe-425e-ba12-4536236cd72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3cols=transformed_df2[['dist_aeroporto','dist_nearist_attraction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3f4df-2d82-42a6-8cb1-ec78b42fb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3df=transformed_df.join(exp3cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1adac29-d4e2-4584-a1dc-35ed86070013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X_exp3 = exp3df.drop('price', axis=1)\n",
    "y = exp3df.price\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_exp3 = pd.DataFrame(scaler.fit_transform(X_exp3), columns=list(X_exp3.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6d0ba-b1c8-418b-8eed-5b13b0969ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train_exp3, X_test_exp3, y_train3, y_test3 = train_test_split(X_exp3, y, test_size=0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3417fd6-ffb8-4937-9211-7b8c7806d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review = list(X_train_exp3.columns[X_train_exp3.columns.str.startswith(\"review_scores\")])\n",
    "review_to_keep = list(X_train_exp3.columns[X_train_exp3.columns.str.startswith(\"review_scores_rating\")])\n",
    "review_to_drop = [x for x in all_review if x not in review_to_keep]\n",
    "all_times_since=list(X_train_exp3.columns[X_train_exp3.columns.str.startswith(\"time_since\")])\n",
    "\n",
    "\n",
    "X_train_exp3 = X_train_exp3.drop(review_to_drop, axis=1)\n",
    "X_train_exp3 = X_train_exp3.drop(all_times_since, axis=1)\n",
    "X_test_exp3 = X_test_exp3.drop(review_to_drop, axis=1)\n",
    "X_test_exp3 = X_test_exp3.drop(all_times_since, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80f925-3027-4ab8-ac24-e09bc12d33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "gbr_regExp3= GradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "#xgb_reg = xgb.XGBRegressor()\n",
    "gbr_regExp3.fit(X_train_exp3, y_train3)\n",
    "training_preds_gbr_regExp3 = gbr_regExp3.predict(X_train_exp3)\n",
    "val_preds_gbr_regExp3 = gbr_regExp3.predict(X_test_exp3)\n",
    "\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train3, training_preds_gbr_regExp3),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test3, val_preds_gbr_regExp3),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train3, training_preds_gbr_regExp3),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test3, val_preds_gbr_regExp3),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88dd6fb-02c4-4478-9e06-d0c946aa242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test3, val_preds_gbr_regExp3)) * (len(y_test3)-1)/(len(y_test3)-X_test_exp3.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ad6f3-ed79-460e-adca-a1a9e3a6754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_gbr_exp3 = pd.DataFrame(gbr_regExp3.feature_importances_, columns=['weight'], index=X_train_exp3.columns)\n",
    "ft_weights_gbr_exp3.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_gbr_exp3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd5f21-68d6-4e4d-85ba-a72eba830eb8",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55521b-0d5b-4dd3-9a3b-93de88a87b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "hpm_regExp3 = LinearRegression()  \n",
    "hpm_regExp3.fit(X_train_exp3, y_train3) #training the algorithm\n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "training_preds_hpm_regExp3 = hpm_regExp3.predict(X_train_exp3)\n",
    "val_preds_hpm_regExp3 = hpm_regExp3.predict(X_test_exp3)\n",
    "\n",
    "\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train3, training_preds_hpm_regExp3),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test3, val_preds_hpm_regExp3),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train3, training_preds_hpm_regExp3),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test3, val_preds_hpm_regExp3),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caefe67-4ad9-4370-8e79-898483a65d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test3, val_preds_hpm_regExp3)) * (len(y_test3)-1)/(len(y_test3)-X_test_exp3.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb0b01-e268-4c44-91d2-7ab796c06a84",
   "metadata": {},
   "source": [
    "* **airbnb+ dist_nearist_attraction + dist_aeroporto + atm_no**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab9a09-a73c-48a3-aa70-f64a2e4f25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp4cols=transformed_df2[['dist_aeroporto','dist_nearist_attraction','atm_no']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870a82f-4806-40de-bef1-4ef2d4ff1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp4df=transformed_df.join(exp4cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14bba3-2a59-4ef9-b1c2-040e1b22d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X_exp4 = exp4df.drop('price', axis=1)\n",
    "y = exp4df.price\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_exp4 = pd.DataFrame(scaler.fit_transform(X_exp4), columns=list(X_exp4.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32fee5c-886d-40d0-928b-83de93209bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train_exp4, X_test_exp4, y_train4, y_test4 = train_test_split(X_exp4, y, test_size=0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4b5e1-7511-473e-8061-6ad2f6de7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review = list(X_train_exp4.columns[X_train_exp4.columns.str.startswith(\"review_scores\")])\n",
    "review_to_keep = list(X_train_exp4.columns[X_train_exp4.columns.str.startswith(\"review_scores_rating\")])\n",
    "review_to_drop = [x for x in all_review if x not in review_to_keep]\n",
    "all_times_since=list(X_train_exp4.columns[X_train_exp4.columns.str.startswith(\"time_since\")])\n",
    "\n",
    "\n",
    "X_train_exp4 = X_train_exp4.drop(review_to_drop, axis=1)\n",
    "X_train_exp4 = X_train_exp4.drop(all_times_since, axis=1)\n",
    "X_test_exp4 = X_test_exp4.drop(review_to_drop, axis=1)\n",
    "X_test_exp4 = X_test_exp4.drop(all_times_since, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7c8d5-0c46-45c1-a90c-3a32daf725a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "gbr_regExp4= GradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "#xgb_reg = xgb.XGBRegressor()\n",
    "gbr_regExp4.fit(X_train_exp4, y_train4)\n",
    "training_preds_gbr_regExp4 = gbr_regExp4.predict(X_train_exp4)\n",
    "val_preds_gbr_regExp4 = gbr_regExp4.predict(X_test_exp4)\n",
    "\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train4, training_preds_gbr_regExp4),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test4, val_preds_gbr_regExp4),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train4, training_preds_gbr_regExp4),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test4, val_preds_gbr_regExp4),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02959b6c-7de4-45a8-bdb9-a5407b476f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test4, val_preds_gbr_regExp4)) * (len(y_test4)-1)/(len(y_test4)-X_test_exp4.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11479a7a-464b-4abb-9be4-60d740b0c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_gbr_exp4 = pd.DataFrame(gbr_regExp4.feature_importances_, columns=['weight'], index=X_train_exp4.columns)\n",
    "ft_weights_gbr_exp4.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_gbr_exp4.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce76f74-36f6-4dc8-b7d0-73565daa24f6",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344d572-0bff-48b1-b515-774e807f0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "hpm_regExp4 = LinearRegression()  \n",
    "hpm_regExp4.fit(X_train_exp4, y_train4) #training the algorithm\n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "training_preds_hpm_regExp4 = hpm_regExp4.predict(X_train_exp4)\n",
    "val_preds_hpm_regExp4 = hpm_regExp4.predict(X_test_exp4)\n",
    "\n",
    "\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train4, training_preds_hpm_regExp4),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test4, val_preds_hpm_regExp4),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train4, training_preds_hpm_regExp4),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test4, val_preds_hpm_regExp4),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4493d-e960-4536-a0e2-8da11c138755",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test4, val_preds_hpm_regExp4)) * (len(y_test4)-1)/(len(y_test4)-X_test_exp4.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6946e-1c02-448b-8416-b8a965de28e3",
   "metadata": {},
   "source": [
    "* **airbnb+ dist_nearist_attraction + dist_aeroporto + atm_no + metro_no**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99316051-827b-4c4f-a376-75838282e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp5cols=transformed_df2[['dist_aeroporto','dist_nearist_attraction','atm_no','metro_no']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8562aa-cb6c-438d-a29c-3115316e5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp5df=transformed_df.join(exp5cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e76a3-d11d-4025-be23-3355a9968b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Separating X and y\n",
    "X_exp5 = exp5df.drop('price', axis=1)\n",
    "y = exp5df.price\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_exp5 = pd.DataFrame(scaler.fit_transform(X_exp5), columns=list(X_exp5.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24485723-931f-410a-8bfb-f07e78df1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train_exp5, X_test_exp5, y_train5, y_test5 = train_test_split(X_exp5, y, test_size=0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feddd662-c785-4d34-93a2-bcc14b2caafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review = list(X_train_exp5.columns[X_train_exp5.columns.str.startswith(\"review_scores\")])\n",
    "review_to_keep = list(X_train_exp5.columns[X_train_exp5.columns.str.startswith(\"review_scores_rating\")])\n",
    "review_to_drop = [x for x in all_review if x not in review_to_keep]\n",
    "all_times_since=list(X_train_exp5.columns[X_train_exp5.columns.str.startswith(\"time_since\")])\n",
    "\n",
    "\n",
    "X_train_exp5 = X_train_exp5.drop(review_to_drop, axis=1)\n",
    "X_train_exp5 = X_train_exp5.drop(all_times_since, axis=1)\n",
    "X_test_exp5 = X_test_exp5.drop(review_to_drop, axis=1)\n",
    "X_test_exp5 = X_test_exp5.drop(all_times_since, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fc11b-64b5-4345-a827-1e04a65a6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "gbr_regExp5= GradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "#xgb_reg = xgb.XGBRegressor()\n",
    "gbr_regExp5.fit(X_train_exp5, y_train5)\n",
    "training_preds_gbr_regExp5 = gbr_regExp5.predict(X_train_exp5)\n",
    "val_preds_gbr_regExp5 = gbr_regExp5.predict(X_test_exp5)\n",
    "\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train5, training_preds_gbr_regExp5),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test5, val_preds_gbr_regExp5),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train5, training_preds_gbr_regExp5),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test5, val_preds_gbr_regExp5),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f1372-fc82-432e-9bf1-7a3d8c3dd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test5, val_preds_gbr_regExp5)) * (len(y_test5)-1)/(len(y_test5)-X_test_exp5.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7f072-24ce-4781-907b-3885ffbf8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_gbr_exp5 = pd.DataFrame(gbr_regExp5.feature_importances_, columns=['weight'], index=X_train_exp5.columns)\n",
    "ft_weights_gbr_exp5.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_gbr_exp5.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982bcdb-eef4-478a-b2c0-b19813ecef28",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d8531-0f66-4e76-949d-1bd89990dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "hpm_regExp5 = LinearRegression()  \n",
    "hpm_regExp5.fit(X_train_exp5, y_train5) #training the algorithm\n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "training_preds_hpm_regExp5 = hpm_regExp5.predict(X_train_exp5)\n",
    "val_preds_hpm_regExp5 = hpm_regExp5.predict(X_test_exp5)\n",
    "\n",
    "\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train5, training_preds_hpm_regExp5),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test5, val_preds_hpm_regExp5),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train5, training_preds_hpm_regExp5),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test5, val_preds_hpm_regExp5),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ce9b0-dfb8-4192-b4c6-6fa654832c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test5, val_preds_hpm_regExp5)) * (len(y_test5)-1)/(len(y_test5)-X_test_exp5.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e674b5b-ae62-43d6-a76a-986d8b9b3112",
   "metadata": {},
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3869d9-9540-4993-8a62-998316338d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df2= transformed_df.join(columns_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b78af-8ed2-473f-a588-ea33186fd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list=['dist_aeroporto','dist_nearist_attraction','atm_no','bars_and_discos_no','metro_no']\n",
    "\n",
    "#plotting distribution of  numeric variables\n",
    "\n",
    "# criando objeto para número de linhas e colunas\n",
    "nrows = 1\n",
    "ncols = 5\n",
    "\n",
    "# definindo área de plotagem\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(30,10))\n",
    "fig.subplots_adjust(hspace=1, wspace=1)\n",
    "\n",
    "# criando loop para plotagem\n",
    "idx = 0\n",
    "for col in columns_list:\n",
    "    idx += 1\n",
    "    plt.subplot(nrows, ncols, idx)\n",
    "    sns.kdeplot(transformed_df2[col], shade=True)\n",
    "    plt.title(col, fontsize=10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c5851-9b3d-4cdc-8ce9-8e002a3b9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_df2 = transformed_df2.loc[(transformed_df2.atm_no < 1000) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46646582-d130-4c0c-b729-c4cefbe85feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_df2 = transformed_df2.loc[(transformed_df2.metro_no < 25) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b442c-06e7-47f1-9f66-0f5af319787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_df2 = transformed_df2.loc[(transformed_df2.bars_and_discos_no < 1000) ]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ec27de0-3150-4f66-a3d7-4d4020c3cce6",
   "metadata": {},
   "source": [
    "transformed_df2.loc[transformed_df2['atm_no'] > 1000, 'atm_no'] = 1000\n",
    "transformed_df2.loc[transformed_df2['metro_no'] > 25, 'metro_no'] = 25\n",
    "transformed_df2.loc[transformed_df2['bars_and_discos_no'] > 1000, 'bars_and_discos_no'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b5f18-8ded-4a0c-a773-f4b5639f5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list=['dist_aeroporto','dist_nearist_attraction','atm_no','bars_and_discos_no','metro_no']\n",
    "\n",
    "#plotting distribution of  numeric variables\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "# criando objeto para número de linhas e colunas\n",
    "nrows = 1\n",
    "ncols = 5\n",
    "\n",
    "# definindo área de plotagem\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10,5))\n",
    "fig.subplots_adjust(hspace=1, wspace=1)\n",
    "\n",
    "# criando loop para plotagem\n",
    "idx = 0\n",
    "for col in columns_list:\n",
    "    idx += 1\n",
    "    plt.subplot(nrows, ncols, idx)\n",
    "    sns.kdeplot(transformed_df2[col], shade=True)\n",
    "    plt.title(col, fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3541489-946b-403a-a93f-8d52b15b81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "statisticsss=transformed_df2[['dist_aeroporto','dist_nearist_attraction','atm_no','bars_and_discos_no','metro_no']].describe()\n",
    "statisticsss\n",
    "#statisticsss.to_csv('stsnewdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2126b1-5284-4ca8-ad92-cdfef600b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X_extended = transformed_df2.drop('price', axis=1)\n",
    "y = transformed_df2.price\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_extended = pd.DataFrame(scaler.fit_transform(X_extended), columns=list(X_extended.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e625b3-84dc-46b5-8796-809b790f6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train_extended, X_test_extended, y_train, y_test = train_test_split(X_extended, y, test_size=0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "db8523a7-60ac-4c95-9a62-7334eef10cfc",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_extended = pd.DataFrame(scaler.fit_transform(X_train_extended), columns=list(X_train_extended.columns))\n",
    "X_test_extended  = pd.DataFrame(scaler.fit_transform(X_test_extended ), columns=list(X_test_extended.columns))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07bb4f32-6120-42e8-8b1a-a8c05722a522",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "xgb_reg2= xgb.XGBRegressor()\n",
    "\n",
    "xgb_reg2.fit(X_train_extended, y_train)\n",
    "training_preds_xgb_reg2 = xgb_reg2.predict(X_train_extended)\n",
    "val_preds_xgb_reg2 = xgb_reg2.predict(X_test_extended)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTraining MSE:\", round(mean_squared_error(y_train, training_preds_xgb_reg2),4))\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_preds_xgb_reg2),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_xgb_reg2),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_xgb_reg2),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e02b7-14be-42dd-ab7d-0ce558b0c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review = list(X_train_extended.columns[X_train_extended.columns.str.startswith(\"review_scores\")])\n",
    "review_to_keep = list(X_train_extended.columns[X_train_extended.columns.str.startswith(\"review_scores_rating\")])\n",
    "review_to_drop = [x for x in all_review if x not in review_to_keep]\n",
    "all_times_since=list(X_train_extended.columns[X_train_extended.columns.str.startswith(\"time_since\")])\n",
    "\n",
    "\n",
    "X_train_extended = X_train_extended.drop(review_to_drop, axis=1)\n",
    "X_train_extended = X_train_extended.drop(all_times_since, axis=1)\n",
    "X_test_extended = X_test_extended.drop(review_to_drop, axis=1)\n",
    "X_test_extended = X_test_extended.drop(all_times_since, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a2dfd-9384-40af-bfff-43234ec10042",
   "metadata": {},
   "source": [
    "### Model 5- LR with extended data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840137b2-1a09-45eb-b245-8df946d34033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "hpm_reg3 = LinearRegression()  \n",
    "hpm_reg3.fit(X_train_extended, y_train) #training the algorithm\n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "training_preds_hpm_reg3 = hpm_reg3.predict(X_train_extended)\n",
    "val_preds_hpm_reg3 = hpm_reg3.predict(X_test_extended)\n",
    "\n",
    "\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train, training_preds_hpm_reg3),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test, val_preds_hpm_reg3),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_hpm_reg3),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_hpm_reg3),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f675a38d-8e3b-4d72-95ba-2f7a8b2457ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extendedLR=X_train_extended.assign(y_train=np.exp(y_train),y_pred_train=np.exp(training_preds_hpm_reg3),error_model=abs(y_train-y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f11ac-f76e-433a-a112-59111b6af79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo a área de plotagem\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "# plotando o gráfico\n",
    "ax = sns.scatterplot(data=X_train_extendedLR, y=\"latitude\", x=\"longitude\", hue='error_model')\n",
    "ax.set_title('Mapping the errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744748b-5ee8-4d3e-ac4d-ff69b48e3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted R-squared\n",
    "#1 - ( 1-hpm_reg3.score(X_train_extended, y_train) ) * ( len(y_train) - 1 ) / ( len(y_train) - X_train_extended.shape[1] - 1 )\n",
    "Adj_r2 = 1 - (1-r2_score(y_test, val_preds_hpm_reg3)) * (len(y_test)-1)/(len(y_test)-X_test_extended.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b06b9-b5ca-4722-abbc-739db41b8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "cdf = pd.DataFrame(hpm_reg3.coef_, X_train_extended.columns, columns=['Coefficients'])\n",
    "print(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9efa2-556c-4255-92f0-58465ac18f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "cdf['Coefficients'] = cdf.apply(lambda row: np.exp(row['Coefficients'])-1, axis=1) #fazer ifelse para as colunas q tao em log e falta a significancia dos coeficientes\n",
    "cdf\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca36b3b1-4fe2-4240-8ceb-d7214c56cc83",
   "metadata": {},
   "source": [
    "### MOdel 6- gradient boosting with extended data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a70608-108b-4c68-a709-8ea2b9704c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "gbr_reg3= GradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "#xgb_reg = xgb.XGBRegressor()\n",
    "gbr_reg3.fit(X_train_extended, y_train)\n",
    "training_preds_gbr_reg3 = gbr_reg3.predict(X_train_extended)\n",
    "val_preds_gbr_reg3 = gbr_reg3.predict(X_test_extended)\n",
    "\n",
    "print(\"\\nTraining MAE:\", round(mean_absolute_error(y_train, training_preds_gbr_reg3),4))\n",
    "print(\"Validation MAE:\", round(mean_absolute_error(y_test, val_preds_gbr_reg3),4))\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_gbr_reg3),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_gbr_reg3),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09ff56-b74e-49c5-9b5b-63cfc7f2f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted R-squared\n",
    "Adj_r2 = 1 - (1-r2_score(y_test, val_preds_gbr_reg3)) * (len(y_test)-1)/(len(y_test)-X_test_extended.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfc2ac-f997-4de7-8ad1-9539f1f94f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_gbr_reg3 = pd.DataFrame(gbr_reg3.feature_importances_, columns=['weight'], index=X_train_extended.columns)\n",
    "ft_weights_gbr_reg3.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_gbr_reg3.head(10)\n",
    "#ft_weights_gbr_reg3.head(10).to_csv('feature_importance2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd56250-f098-4abe-983b-465c52a784dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importances\n",
    "plt.figure(figsize=(10,25))\n",
    "plt.barh(ft_weights_gbr_reg3.index, ft_weights_gbr_reg3.weight, align='center') \n",
    "plt.title(\"Feature importances in the Gradient Boosted Trees model\", fontsize=14)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.margins(y=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0989e2-4ca4-46a3-ae6b-fc720dbdc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test_extended.columns\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(gbr_reg3)  # model used \n",
    "\n",
    "shap_values = explainer.shap_values(X_test_extended.iloc[700]) # predicting 6 row of the test dataset\n",
    "shap.initjs()\n",
    "\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=shap_values,\n",
    "    features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d40f1-371b-4164-a612-9de866e29341",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test_extended.columns\n",
    "explainer = shap.TreeExplainer(gbr_reg3)  # model used \n",
    "shap_values = explainer(X_test_extended) \n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b830f5-7579-4d4e-b7ee-3a1f253b499c",
   "metadata": {},
   "source": [
    "### Final model selection- hypertunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e8fda-37ab-4c61-a54c-802d6880ca3a",
   "metadata": {},
   "source": [
    "Overall, the Gradient Boosting model with extended data (Model 6) is the preferred model, which performs better than both Spatial Hedonic Regression Models and just as good as the first model but is less computationally expensive. It will be improved further with hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d34e50-6719-4b71-867f-0c003fca1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2705ddfa-dcb1-4ee8-aefc-c0dbb4fbc95a",
   "metadata": {},
   "source": [
    "\n",
    "# instanciando o algoritmo para tunning\n",
    "crossvalidation=KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "gbr=GradientBoostingRegressor(random_state=seed)\n",
    "\n",
    "search_grid={'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1],'max_depth':[1,2,4],'subsample':[.5,.75,1],'random_state':[1]}\n",
    "search=GridSearchCV(estimator=gbr,param_grid=search_grid,scoring='neg_mean_squared_error',n_jobs=1,cv=crossvalidation)\n",
    "\n",
    "search.fit(X_train_extended,y_train)\n",
    "search.best_params_\n",
    "\n",
    "# treinando o modelo com RandomSearch\n",
    "#gbr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f11cbac-8f37-409e-9285-8bc8c4531e79",
   "metadata": {},
   "source": [
    "# instanciando com os melhores hiperparâmetros\n",
    "best_random  = search.best_estimator_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a75c0c42-fdfa-450f-9a6e-2bc423b77e6f",
   "metadata": {},
   "source": [
    "# treinando com os melhores hiperparâmetros\n",
    "best_random.fit(X_train_extended, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22238526-deee-4c9c-8e09-15c61acc2dec",
   "metadata": {},
   "source": [
    "import joblib\n",
    "filename = \"Completed_model.joblib\"\n",
    "joblib.dump(best_random, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac029f7-f155-4674-99ea-8b24ac38c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "filename = \"Completed_model.joblib\"\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test_extended, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d60697-ebf9-47eb-9b65-d8e3427d6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizando as previsões\n",
    "y_pred_train = loaded_model.predict(X_train_extended)\n",
    "y_pred_test= loaded_model.predict(X_test_extended)\n",
    "\n",
    "print(f'Previsão nos dados de treino:')\n",
    "print('-----------------------------------------------------')\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_train, y_pred_train)}')\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(y_train, y_pred_train)}')\n",
    "print(f'R2 score: {r2_score(y_train, y_pred_train)}\\n')\n",
    "\n",
    "print(f'Previsão nos dados de Validação:')\n",
    "print('-----------------------------------------------------')\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred_test)}')\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(y_test, y_pred_test)}')\n",
    "print(f'R2 score: {r2_score(y_test, y_pred_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366350f-56e3-4c29-bee7-ceac39f8f7b6",
   "metadata": {},
   "source": [
    "### Plot the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1f9c7-db46-492b-8f26-2767a634a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extended2=X_train_extended.assign(y_train=np.exp(y_train),y_pred_train=np.exp(y_pred_train),error_model=abs(y_train-y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb792d-bf2b-4fad-a04f-446fbb93c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extended2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99acc6-dd2e-4819-bd3a-f81ce1f9cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train_extended2.groupby('property_type_Other')['error_model'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2c821-286a-44d8-b856-ee70374b830b",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train_extended2.groupby('room_type_Shared room')['error_model'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f5bdc-4003-4762-86ef-e0b529c6aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train_extended2.groupby('room_type_Entire home/apt')['error_model'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128f3c4-0693-4e4e-b0e3-9214e5ab975e",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train_extended2.groupby('room_type_Hotel room')['error_model'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5bcbd-520f-4e05-9b3e-33919bc6e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(X_train_extended2, values=[0.181281,0.184886,0.181281], names=['Shared room','Entire home/apartment','Hotel room'], title='Error per room type')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba492cef-6462-47fa-a11a-e980df951bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(X_train_extended2, values=[0.188084,0.178353], names=['House','Other'], title='Error per property type')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a8412-780e-4e0a-a22b-bbb8368da4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.labelsize'] = 20\n",
    "# definindo a área de plotagem\n",
    "fig, ax = plt.subplots(figsize = (12,10))\n",
    "\n",
    "# plotando o gráfico\n",
    "ax = sns.scatterplot(data=X_train_extended2, y=\"latitude\", x=\"longitude\", hue='error_model')\n",
    "ax.set_title('Mapping the errors', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e8a39-d395-4c34-a7e7-f075e52082fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_r2 = 1 - (1-r2_score(y_test, y_pred_test)) * (len(y_test)-1)/(len(y_test)-X_test_extended.shape[1]-1)\n",
    "Adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4827ff-e6c4-42d3-8df9-af613a33381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando a área de plotagem\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# plotando os gráficos\n",
    "sns.regplot(x=y_train, y=y_pred_train, ax=ax1)\n",
    "\n",
    "ax1.grid(axis='y')\n",
    "ax1.set_xlabel('Dados atuais')\n",
    "ax1.set_ylabel('Dados previstos')\n",
    "ax1.set_title('Previsão nos dados de TREINO')\n",
    "plt.setp(ax1.get_xticklabels(), rotation=45);\n",
    "\n",
    "sns.regplot(x=y_test, y=y_pred_test, ax=ax2)\n",
    "\n",
    "ax2.grid(axis='y')\n",
    "ax2.set_xlabel('Dados atuais')\n",
    "ax2.set_ylabel('Dados previstos')\n",
    "ax2.set_title('Previsão nos dados de VALIDAÇÃO')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b16bf4-754f-44ab-8b26-ad9b9474c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_gbr_regT = pd.DataFrame(loaded_model.feature_importances_, columns=['weight'], index=X_test_extended.columns)\n",
    "ft_weights_gbr_regT.sort_values('weight', ascending=False, inplace=True)\n",
    "ft_weights_gbr_regT.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510e0b2-3b76-4843-a92b-3b516a5bb5b7",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0b560-20f4-4733-a6a4-9347f46b6c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test_extended.columns\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(loaded_model)  # model used \n",
    "\n",
    "shap_values = explainer.shap_values(X_test_extended.iloc[1304]) # predicting 700th row of the test dataset\n",
    "shap.initjs()\n",
    "\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=shap_values,\n",
    "    features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8800a296-c16d-43d2-8159-4936d4ff9680",
   "metadata": {},
   "source": [
    "The force plot is another way to see the effect each feature has on the prediction, for a given observation. In this plot the positive SHAP values are displayed on the left side and the negative on the right side, as if competing against each other.  Features in red color influence positiv, features in blue color - the opposite. \n",
    "The base value is the average of all output values of the model on the training\n",
    "The highlighted value is the prediction for that observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be67181-cf97-4db8-9730-5f7e6217f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(loaded_model)  # model used \n",
    "\n",
    "shap_values = explainer(X_test_extended) \n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97a054-234d-450b-ad13-cdc47a1361ab",
   "metadata": {},
   "source": [
    "On the beeswarm the features are also ordered by their effect on prediction, but we can also see how higher and lower values of the feature will affect the result.\n",
    "\n",
    "All the little dots on the plot represent a single observation. The horizontal axis represents the SHAP value, while the color of the point shows us if that observation has a higher or a lower value, when compared to other observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b8269-13cc-4712-8e3d-8b32533f78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c1719-522d-45ae-8d18-f8980b2d9f57",
   "metadata": {},
   "source": [
    "Here the features are ordered from the highest to the lowest effect on the prediction. It takes in account the absolute SHAP value, so it does not matter if the feature affects the prediction in a positive or negative way.\n",
    "This is a mean SHAP plot. For each feature, we calculate the mean of the absolute SHAP values across all observations. We take the absolute values as we do not want positive and negative values to offset each other.  There is one bar for each feature and we can see that accomoodates weight had the largest mean SHAP out of all the features.\n",
    "\n",
    "Features that have large mean SHAP values will tend to have large positive/negative SHAP values. In other words, these are the features that have a significant impact on the model’s predictions. In this sense, this plot can be used in the same way as a feature importance plot. That is to highlight features that are important to a model’s predictions. An issue is that it does not tell us anything about the nature of the relationship between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac839a-2e3c-4547-a4e5-8693e9604d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d9725-136d-4031-8381-24e33b9396b5",
   "metadata": {},
   "source": [
    "For analysis of local, instance-wise effects, we can use the before plots on single observations (in the examples above I used shap_values[0]).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e036a886-02be-4fcb-9a1c-20efa43a0b43",
   "metadata": {},
   "source": [
    "This plot shows us what are the main features affecting the prediction of a single observation, and the magnitude of the SHAP value for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ef927-546d-4376-a304-ede001c79e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get expected value and shap values array\n",
    "expected_value = explainer.expected_value\n",
    "shap_array = explainer.shap_values(X_test_extended)\n",
    "\n",
    "#Descion plot for first 10 observations\n",
    "shap.decision_plot(expected_value, shap_array[0:10],feature_names=list(X_test_extended.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfdcf9-5991-431b-8be0-757a2db8bcd6",
   "metadata": {},
   "source": [
    "Waterfall and force plots are great for interpreting individual predictions. To understand how our model makes predictions in general we need to aggregate the SHAP values. One way to do this is using a decision plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3eeac3-a538-4036-9d08-7ac772f7196a",
   "metadata": {},
   "source": [
    "## Responsible Machine Learning with Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53e724-ea2f-4f1a-a561-d1553f9492ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install interpret-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09026c-f8eb-4010-ac5b-1850200f1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install raiwidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac572f-6833-471c-9a3e-da9a79c4e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pip install –e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2437d20-324d-4e94-ae3f-50e57f21b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import raiwidgets\n",
    "from raiwidgets import ErrorAnalysisDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778268b-2546-468c-a445-0d5dcfbe27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train.copy()\n",
    "test_data = X_test.copy()\n",
    "train_data['price'] = y_train\n",
    "test_data['price'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a097d8-7a5e-4c71-ba14-2b9a6321ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = train_data.select_dtypes(\"object\").columns\n",
    "\n",
    "catCols= list(set(catCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e35f0-7bf2-4d89-ab36-14bfc45904fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=X_test_extended.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877836f5-747a-4c63-8237-332c7f80b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features=features.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e029d6-7284-4cd0-a6ec-b9fe425649ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_extended.values.astype(np.float)\n",
    "y = y_test.values.astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d535565-13d2-43b5-807b-a33f0189972c",
   "metadata": {},
   "source": [
    "### Load simple ErrorAnalysis view without explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824dce6-a6f1-487b-882d-c91a92cee6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ErrorAnalysisDashboard(dataset=X, true_y=y, features=features, pred_y=y_pred_test, model_task='regression', locale='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e5f52-3586-4687-80c6-51c8f2e49e7c",
   "metadata": {},
   "source": [
    "### Train a surrogate model to explain the original blackbox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4b0d6-695a-41ab-a6bf-b9ea2cd8f4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca966ada-1f20-4e35-a69a-a9d77c6b6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for SHAP MimicExplainer with LightGBM surrogate model\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import LGBMExplainableModel\n",
    "\n",
    "from interpret_community.common.constants import ModelTask\n",
    "features=X_train_extended.columns\n",
    "# Train the LightGBM surrogate model using MimicExplaner\n",
    "model_task = ModelTask.Regression\n",
    "explainer = MimicExplainer(loaded_model, X_train_extended, LGBMExplainableModel,\n",
    "                           augment_data=True, max_num_of_augmentations=10,\n",
    "                           features=features, model_task=model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8276f-811f-4aa0-af04-707f333ec5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# X_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "global_explanation = explainer.explain_global(X_test_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314aa87c-7451-46ea-8a9b-dbdfbb7975cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_extended.index = X_test_extended.index.astype(int) #use astype to convert to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f65537-eca7-4cb6-97d0-95fe1738627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysisDashboard(global_explanation, loaded_model, dataset=X_test_extended, true_y=y, model_task='regression',locale='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c94910-4082-47c6-a2ed-cdc0d039f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
